{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY1lXPCAJg2E"
   },
   "source": [
    "# PEFT tutorial using Hugging Face\n",
    "## æ•™å­¸ç›®æ¨™\n",
    "åˆ©ç”¨ Hugging Face å¥—ä»¶å¿«é€Ÿä½¿ç”¨ PEFT ä¾†é€²è¡Œä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
    "- å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNg9uH2-Jg2H"
   },
   "source": [
    "## é©ç”¨å°è±¡\n",
    "å·²ç¶“æœ‰åŸºæœ¬çš„æ©Ÿå™¨å­¸ç¿’çŸ¥è­˜ï¼Œä¸”æ“æœ‰ Pythonã€`numpy`ã€`pandas`ã€`scikit-learn` ä»¥åŠ `PyTorch` åŸºç¤çš„å­¸ç”Ÿã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é Pythonï¼Œè«‹åƒè€ƒ [python-å…¥é–€èªæ³•](./python-å…¥é–€èªæ³•.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `pandas`ï¼Œè«‹åƒè€ƒ [pandas-åŸºæœ¬åŠŸèƒ½](./pandas-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `numpy`ï¼Œè«‹åƒè€ƒ [numpy-åŸºæœ¬åŠŸèƒ½](./numpy-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é `scikit-learn`ï¼Œè«‹åƒè€ƒ [scikit-learn-åŸºæœ¬åŠŸèƒ½](./scikit-learn-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸é  `PyTorch` ï¼Œè«‹åƒè€ƒ [PyTorch-åŸºæœ¬åŠŸèƒ½](./PyTorch-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
    "\n",
    "è‹¥æ²’æœ‰å…ˆå­¸éå¦‚ä½•ä½¿ç”¨ `PyTorch` å»ºç«‹è‡ªç„¶èªè¨€è™•ç†åºåˆ—æ¨¡å‹ï¼Œè«‹åƒè€ƒ [NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡](./NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡.ipynb) æ•™å­¸ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT ç°¡æ˜“ä»‹ç´¹\n",
    "### å°å¤§èªè¨€æ¨¡å‹é€²è¡Œå¾®èª¿çš„æŒ‘æˆ°\n",
    "- å¤§èªè¨€æ¨¡å‹çš„é€šå¸¸æ˜¯ä»¥å¤§é‡çš„æ–‡æœ¬è³‡æ–™é€²è¡Œè¨“ç·´ï¼Œä¸¦ä¸”åœ¨å¤šå€‹ä»»å‹™ä¸Šå–å¾—äº†é©šäººçš„è¡¨ç¾ã€‚\n",
    "- è‹¥æˆ‘å€‘æƒ³è¦å°‡é€™äº›å¤§èªè¨€æ¨¡å‹æ‡‰ç”¨åœ¨è‡ªå·±çš„ä»»å‹™ä¸Šï¼Œé€šå¸¸éœ€è¦é€²è¡Œå¾®èª¿ã€‚\n",
    "- ä½†æ˜¯å°æ–¼å¤§èªè¨€æ¨¡å‹é€²è¡Œå¾®èª¿æ˜¯ä¸€å€‹æŒ‘æˆ°ï¼Œå› ç‚ºé€™äº›æ¨¡å‹é€šå¸¸æœ‰æ•¸åå„„ç”šè‡³æ•¸ç™¾å„„çš„åƒæ•¸ï¼Œä¸¦ä¸”éœ€è¦å¤§é‡çš„è¨ˆç®—è³‡æºã€‚\n",
    "- é€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘éœ€è¦ PEFT é€™å€‹å¥—ä»¶ï¼Œå®ƒå¯ä»¥å¹«åŠ©æˆ‘å€‘å¿«é€Ÿçš„é€²è¡Œå¤§èªè¨€æ¨¡å‹çš„å¾®èª¿ã€‚\n",
    "![](https://i.imgur.com/q6u4GVJ.png)\n",
    "- æ›´å¤šç´°ç¯€è«‹åƒè€ƒ ([Peft github](https://github.com/huggingface/peft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n0nQj0qJg2I"
   },
   "source": [
    "## PEFT ç¯„ä¾‹: LoRA\n",
    "![](https://i.imgur.com/GCsNYXF.png)\n",
    "- è«‹åƒè€ƒç†è«–å±¤é¢çš„è©³ç´°æ•™å­¸ ([å½±ç‰‡é€£çµ](https://www.youtube.com/watch?v=dA-NhCtrrVE))\n",
    "- ä¹Ÿå¯ä»¥åƒè€ƒåŸå§‹è«–æ–‡ ([è«–æ–‡é€£çµ](https://arxiv.org/abs/2106.09685))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT5oltZrJg2J"
   },
   "source": [
    "## Hugging Face ä»‹ç´¹\n",
    "- ğŸ¤— Hugging Face æ˜¯å°ˆé–€æä¾›è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸçš„å‡½å¼åº«\n",
    "- å…¶å‡½å¼åº«æ”¯æ´ PyTorch å’Œ TensorFlow\n",
    "- ğŸ¤— Hugging Face çš„ä¸»è¦å¥—ä»¶ç‚º:\n",
    "    1. Transformers ([é€£çµ](https://huggingface.co/transformers/index.html))\n",
    "    - æä¾›äº†ç¾ä»Šæœ€å¼·å¤§çš„è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šéå¸¸å½ˆæ€§ä¸”æ–¹ä¾¿\n",
    "    2. Tokenizers ([é€£çµ](https://huggingface.co/docs/tokenizers/python/latest/))\n",
    "    - è®“ä½ å¯ä»¥å¿«é€Ÿåšå¥½ BERT ç³»åˆ—æ¨¡å‹ tokenization\n",
    "    3. Datasets ([é€£çµ](https://huggingface.co/docs/datasets/))\n",
    "    - æä¾›å¤šç¨®è‡ªç„¶èªè¨€è™•ç†ä»»å‹™çš„è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUjyDQ-qJg2J",
    "outputId": "90b838b2-f0bb-4c14-fc20-1e92324695b2"
   },
   "outputs": [],
   "source": [
    "# è‹¥æ²’æœ‰å®‰è£ transformers å’Œ datasets å¥—ä»¶ï¼Œè«‹å–æ¶ˆä»¥ä¸‹è¨»è§£ä¸¦åŸ·è¡Œ\n",
    "# !pip install transformers==4.38.0\n",
    "# !pip install datasets\n",
    "# !pip install torch==2.0.1+cu110\n",
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# %cd apex\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install -v --disable-pip-version-check --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3tiYaLoJg2K",
    "outputId": "f9bb0cf9-8aec-404a-9c29-2074542caeb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch çš„ç‰ˆæœ¬ç‚º: 2.1.2+cu118\n",
      "Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: 4.39.3\n",
      "Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: 2.18.0\n",
      "PEFT çš„ç‰ˆæœ¬ç‚º: 0.10.0\n"
     ]
    }
   ],
   "source": [
    "# ç¢ºèªæ‰€éœ€å¥—ä»¶çš„ç‰ˆæœ¬\n",
    "import torch\n",
    "print(\"PyTorch çš„ç‰ˆæœ¬ç‚º: {}\".format(torch.__version__))\n",
    "\n",
    "import transformers\n",
    "print(\"Hugging Face Transformers çš„ç‰ˆæœ¬ç‚º: {}\".format(transformers.__version__))\n",
    "\n",
    "import datasets\n",
    "print(\"Hugging Face Datasets çš„ç‰ˆæœ¬ç‚º: {}\".format(datasets.__version__))\n",
    "\n",
    "import peft\n",
    "print(\"PEFT çš„ç‰ˆæœ¬ç‚º: {}\".format(peft.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s-JMEX4oJg2L"
   },
   "outputs": [],
   "source": [
    "# è¼‰å…¥å…¶ä»–æ‰€éœ€å¥—ä»¶\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path # (Python3.4+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: è³‡æ–™è¼‰å…¥\n",
    "æœ¬æ¬¡ä½œæ¥­è¦æ±‚é¸æ“‡GLUE benchmarkä¸­çš„è³‡æ–™é›†è‡³å°‘å…©å€‹\\\n",
    "ä»¥ä¸Šæ˜¯ä¸€ç¨®è®€å–è³‡æ–™é›†çš„ç¯„ä¾‹ï¼Œä½†å…·é«”æ€éº¼è®€ä¸åšè¦æ±‚ï¼Œå¯ä»¥ç›´æ¥ç”¨huggingfaceçš„load_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW2UFRcGJg2N"
   },
   "source": [
    "## Hugging Face AutoTokenizer\n",
    "- ä½¿ç”¨ AutoTokenizer æ­é… Hugging Face models çš„åç¨±å¯ä»¥ç›´æ¥å‘¼å«ä½¿ç”¨\n",
    "- èˆ‰ä¾‹:\n",
    "    - transformers.AutoTokenizer.from_pretrained('roberta-base')\n",
    "- [é»é€™è£¡ä¾†æŸ¥çœ‹ Hugging Face models çš„åç¨±](https://huggingface.co/transformers/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smRMfe1RJg2N",
    "outputId": "216957f3-89c2-45cd-f463-560ddf7c20ce"
   },
   "outputs": [],
   "source": [
    "# è¼‰å…¥ tokenizer\n",
    "\n",
    "# åœ¨ Hugging Face å¥—ä»¶ä¸­å¯ä½¿ç”¨ .from_pretrained() çš„æ–¹æ³•ä¾†å°å…¥é è¨“ç·´æ¨¡å‹\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlHobatFJg2O"
   },
   "source": [
    "## Hugging Face Datasets\n",
    "- Hugging Face Datasets å·²ç¶“å¹«ä½ æ”¶éŒ„äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå¸¸è¦‹çš„è³‡æ–™é›†\n",
    "- ç›´æ¥å‘¼å« Datasets ä¸¦æ­é…ä¸‹é¢å¹¾å€‹ cells çš„èªæ³•ï¼Œå¯çœä¸‹ä¸å°‘æ™‚é–“\n",
    "- ä½†å‰ææ˜¯ä½ è¦é€²è¡Œçš„ä»»å‹™è³‡æ–™é›†æœ‰è¢«æ”¶éŒ„åœ¨ Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLUE benchmark - CoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o5nX6CEYJg2O"
   },
   "outputs": [],
   "source": [
    "# å¾ Hugging Face Datasets è¼‰å…¥è³‡æ–™ä¸¦åšè³‡æ–™åˆ‡åˆ†\n",
    "\n",
    "# è¼‰å…¥ CoLA çš„è¨“ç·´è³‡æ–™é›†\n",
    "train_cola = datasets.load_dataset(\"glue\", \"cola\", split=\"train\", cache_dir=\"./cache/cola/train\")\n",
    "\n",
    "# è¼‰å…¥ CoLA çš„é©—è­‰è³‡æ–™é›†\n",
    "valid_cola = datasets.load_dataset(\"glue\", \"cola\", split=\"validation\", cache_dir=\"./cache/cola/valid\")\n",
    "\n",
    "# è¼‰å…¥ CoLA çš„æ¸¬è©¦è³‡æ–™é›†\n",
    "test_cola = datasets.load_dataset(\"glue\", \"cola\", split=\"test\", cache_dir=\"./cache/cola/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU4Zkxx2Jg2O",
    "outputId": "48fe75f5-70f8-4eb1-8765-a245d39de219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551 1043 1063\n"
     ]
    }
   ],
   "source": [
    "print(len(train_cola), len(valid_cola), len(test_cola))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cola.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDGsIlStJg2O",
    "outputId": "80f30607-bf84-4b7e-a03c-f9b932d1cec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹ max_length\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "08040ed6406841cdb419bc520c27b0aa",
      "d92e9c53c0cc4b38afefb821b615b01c",
      "08319e9ed1b9480baae1a5d697b8cc10",
      "b86ee9c64f494d7bbefab8b86bc84da3",
      "3938d967b7414535ba6b5011dc56470f",
      "1dfed01ed6bf4f04994cf26f439380f7",
      "23697ab45e464e31bbf0f7e3883dce11",
      "bcdeab999f3b48a9aae8c12420b2bfcb",
      "daaf705153f7406682246dfe5d7321f7",
      "409db3f8a55a4cd28696e66074773aab",
      "f36b9796a29f45d980986087aeb79d42",
      "3a5a32b8db02492abf45cc1a5c111d7c",
      "608e362d2f92485a97e3556fd370957b",
      "ec632d4d34544dd989df07c1c26f2467",
      "00dee68c6ffd43a4b5cf30f09d71bcb0",
      "0ea9133bf4664aee8c6dd74eda254cc5",
      "fae4e545f58e4ca9aa1f49bad91a36d4",
      "fece13fdff59433ebbb3a35320574e5f",
      "87a9ce7ed8fa43988c59bb3d9ced637b",
      "4938681ec83a4338a20288242b762122",
      "f06d935a23144d0eba94969c6ea91cbb",
      "8e95a576f3d34366beb19b5bde93dc5b",
      "b9548eee1f3c45f3b49865e4a625d00f",
      "3299d0d455874ab38b7f3545d9c73e79",
      "7a142a6dd37b46daa64feff35f4168c6",
      "f5de6cf424a54676bb153afa6cfe96a4",
      "1ec526cda13940a6b77fdd623b69f1f7",
      "339e040e2baf4dcf900b6d76abeb2c14",
      "02168fc723b143849b7d8488d4b73997",
      "1fe8a068cd744e8ca846095190e977dc",
      "6df9c93e3e6e4f94b48b1f2df6b350c3",
      "a0bb14037015498794f441bcbac020da",
      "620d401375d7406e8922d7c51246bd31"
     ]
    },
    "id": "mOT5JmqFJg2O",
    "outputId": "6b1b8a2b-6592-4604-b99b-79d6b1ab5451"
   },
   "outputs": [],
   "source": [
    "# å°‡ Hugging Face Datasets è½‰ç‚º PyTorch Dataset çš„å°è£\n",
    "\n",
    "def cola_to_torch_data(hug_dataset):\n",
    "    \"\"\"å°‡ Hugging Face Datasets è½‰ç‚º PyTorch Dataset\n",
    "    Args:\n",
    "        - hug_dataset: å¾ Datasets è¼‰å…¥çš„è³‡æ–™é›†\n",
    "    Return:\n",
    "        - dataset: å·²è½‰ç‚º PyTorch Dataset çš„è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    dataset = hug_dataset.map(\n",
    "        lambda batch: tokenizer(\n",
    "            batch[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=[\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "            'label'\n",
    "        ]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset_cola = cola_to_torch_data(train_cola)\n",
    "val_dataset_cola = cola_to_torch_data(valid_cola)\n",
    "test_dataset_cola = cola_to_torch_data(test_cola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLUE Benchmark - MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾ Hugging Face Datasets è¼‰å…¥è³‡æ–™ä¸¦åšè³‡æ–™åˆ‡åˆ†\n",
    "\n",
    "# è¼‰å…¥ MRPC çš„è¨“ç·´è³‡æ–™é›†\n",
    "train_mrpc = datasets.load_dataset(\"glue\", \"mrpc\", split=\"train\", cache_dir=\"./cache/mrpc/train\")\n",
    "\n",
    "# è¼‰å…¥ MRPC çš„é©—è­‰è³‡æ–™é›†\n",
    "valid_mrpc = datasets.load_dataset(\"glue\", \"mrpc\", split=\"validation\", cache_dir=\"./cache/mrpc/valid\")\n",
    "\n",
    "# è¼‰å…¥ MRPC çš„æ¸¬è©¦è³‡æ–™é›†\n",
    "test_mrpc = datasets.load_dataset(\"glue\", \"mrpc\", split=\"test\", cache_dir=\"./cache/mrpc/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU4Zkxx2Jg2O",
    "outputId": "48fe75f5-70f8-4eb1-8765-a245d39de219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668 408 1725\n"
     ]
    }
   ],
   "source": [
    "print(len(train_mrpc), len(valid_mrpc), len(test_mrpc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mrpc.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡ Hugging Face Datasets è½‰ç‚º PyTorch Dataset çš„å°è£\n",
    "\n",
    "def mrpc_to_torch_data(hug_dataset):\n",
    "    \"\"\"å°‡ Hugging Face Datasets è½‰ç‚º PyTorch Dataset\n",
    "    Args:\n",
    "        - hug_dataset: å¾ Datasets è¼‰å…¥çš„è³‡æ–™é›†\n",
    "    Return:\n",
    "        - dataset: å·²è½‰ç‚º PyTorch Dataset çš„è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    dataset = hug_dataset.map(\n",
    "        lambda batch: tokenizer(\n",
    "            batch[\"sentence1\"],\n",
    "            batch[\"sentence2\"],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=[\n",
    "            'input_ids',\n",
    "            'attention_mask',\n",
    "            'label'\n",
    "        ]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset_mrpc = mrpc_to_torch_data(train_mrpc)\n",
    "val_dataset_mrpc = mrpc_to_torch_data(valid_mrpc)\n",
    "test_dataset_mrpc = mrpc_to_torch_data(test_mrpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfGCHkFpJg2O"
   },
   "source": [
    "## æª¢æŸ¥ tokenization å¾Œçš„çµæœ\n",
    "- ä½¿ç”¨ Hugging Face tokenizer é€²è¡Œ tokenization å¾Œçš„çµæœæ˜¯ä¸€å€‹ dict\n",
    "- é€™å€‹ dict çš„ keys åŒ…å« 'input_ids' å’Œ 'attention_mask'\n",
    "- input_ids: åŸæœ¬å¥å­ä¸­çš„æ¯å€‹å­—è©è¢«æ–·è©å¾Œè½‰æ›æˆå­—å…¸çš„ ID\n",
    "    - æ³¨æ„!! tokenizer å°å°çš„å‹•ä½œå·²ç¶“å¹«ä½ å®Œæˆäº†æ–·è©å’Œ word to ID çš„è½‰æ›\n",
    "- attention_mask: tokenization å¾Œå¥å­ä¸­åŒ…å«æ–‡å­—çš„éƒ¨åˆ†ç‚º 1ï¼Œpadding çš„éƒ¨åˆ†ç‚º 0\n",
    "    - å¯ä»¥æƒ³åƒæˆæ¨¡å‹éœ€è¦æŠŠæ³¨æ„åŠ›æ”¾åœ¨æœ‰æ–‡å­—çš„ä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(1),\n",
       " 'input_ids': tensor([    0,  3762,    55, 38283,   937,  1938,     8,    38,   437,  1311,\n",
       "            62,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_cola[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([    0,   975, 26802,  1588,   102,  2164, 13976,  1758,   128,    29,\n",
       "           137,  2183,     5,  3206,     7, 11881, 10564,    11,  6708,    13,\n",
       "            68,   132,     4,   245,   325,   479,     2,     2,   975, 26802,\n",
       "          1588,   102,  2162, 13976,  1758,   128,    29,    11,  7969,    13,\n",
       "            68,   231,  6478,   153,     8,  1088,    24,     7, 11881, 10564,\n",
       "            13,    68,   112,     4,   398,   325,    11,  6708,   479,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_mrpc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MF4ZQrNJg2O"
   },
   "source": [
    "## ä½¿ç”¨ Hugging Face çš„æ¨¡å‹\n",
    "- åœ¨é€™å€‹ API ç››è¡Œçš„ä¸–ä»£ï¼Œç¸½æ˜¯æœ‰äººå¹«ä½ è¨­æƒ³å‘¨åˆ°\n",
    "- [Hugging Face çš„æ¨¡å‹é é¢é€£çµ](https://huggingface.co/models)\n",
    "- ä»¥ Roberta ç‚ºä¾‹ï¼Œåªè¦é€é AutoModel.from_pretrained(\"roberta-base\")ï¼Œå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ RobertaModel\n",
    "- éœ€è¦æ³¨æ„çš„æ˜¯æ¥ä¸‹ä¾†ä½ è¦åšæ€æ¨£çš„ä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
    "- åŒæ¨£ä»¥ Roberta ç‚ºä¾‹ï¼Œåœ¨åŸå§‹è«–æ–‡ä¸­ Roberta é€²è¡Œéä»¥ä¸‹çš„ä»»å‹™:\n",
    "    - Sentence pair classification: MNLI/QQP/QNLI/MRPC/RTE/WNLI\n",
    "        - å°æ‡‰ `RobertaForSequenceClassification`\n",
    "        - ä½¿ç”¨é›™å¥çµåˆï¼Œä¸¦ä»¥åˆ†é¡çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
    "    - Semantic textual similarity: STS-B\n",
    "        - `RobertaForSequenceClassification`\n",
    "        - ä½¿ç”¨é›™å¥çµåˆï¼Œä¸¦ä»¥è¿´æ­¸çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
    "    - Single sentence classification: SST-2/CoLA\n",
    "        - å°æ‡‰ `RobertaForSequenceClassification`\n",
    "        - ä½¿ç”¨å–®å¥ï¼Œä¸¦ä»¥è¿´æ­¸çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
    "    - Question answering: SQuAD v1.1/v2.0\n",
    "        - å°æ‡‰ `RobertaForQuestionAnswering`\n",
    "        - ä½¿ç”¨é›™å¥(å•é¡Œ+åŸæ–‡)ï¼Œä¸¦é€éç­”æ¡ˆåœ¨åŸæ–‡ä¸­çš„ä½ç½®é€²è¡Œè¨“ç·´\n",
    "    - Named-entity recognition (slot filling): CoNLL-2003\n",
    "        - å°æ‡‰ `RobertaForTokenClassification`\n",
    "        - ä½¿ç”¨å–®å¥ï¼Œä¸¦ä»¥åˆ†é¡çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
    "- å¦‚æœè¦é€²è¡Œçš„ä¸‹æ¸¸ä»»å‹™è¨“ç·´ä¸åœ¨ Hugging Face å·²ç¶“å»ºå¥½çš„æ¨¡å‹ç¯„åœï¼Œé‚£å°±éœ€è¦è‡ªå·±å¯«ä¸€å€‹ model class:\n",
    "    1. ç¹¼æ‰¿ torch.nn.Module\n",
    "    2. åˆ©ç”¨ super ä¾†ç¹¼æ‰¿æ‰€æœ‰è¦ªå±¬é¡åˆ¥çš„å¯¦é«”å±¬æ€§\n",
    "    3. å®šç¾©æ¬²ä½¿ç”¨çš„ pre-trained model\n",
    "    4. å®šç¾©æœƒä½¿ç”¨åˆ°çš„å±¤å¦‚ linear æˆ– Dropout ç­‰\n",
    "    5. è¨­è¨ˆ forward function ä¸¦ä¸”è¨­å®šä¸‹æ¸¸ä»»å‹™çš„è¼¸å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sFWgVD0Jg2P"
   },
   "source": [
    "## é€²è¡Œæ¨¡å‹çš„è¨“ç·´\n",
    "### ä½¿ç”¨ Hugging Face Trainer ([Documentation](https://huggingface.co/transformers/main_classes/trainer.html))\n",
    "- Trainer æ˜¯ Hugging Face ä¸­é«˜åº¦å°è£çš„å¥—ä»¶ä¹‹ä¸€ï¼Œè² è²¬æ¨¡å‹è¨“ç·´æ™‚æœŸçš„\"æµç¨‹\"\n",
    "- éå»æˆ‘å€‘è‡ªè¡Œå¯«è¨“ç·´æµç¨‹çš„ç¨‹å¼ç¢¼å¯ä»¥äº¤çµ¦ Trainer\n",
    "- Trainer éœ€è¦æ­é…ä½¿ç”¨ [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)\n",
    "    - TrainingArguments æ˜¯ Trainer æ‰€éœ€è¦çš„å¼•æ•¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: æ¨¡å‹é©—è­‰\n",
    "é€™è£è¦æ±‚åŒå­¸å€‘æ’°å¯«computer_metricså‡½å¼ \\\n",
    "è¦æ±‚åŒå­¸å€‘åƒè€ƒGLUE benchmarkçš„å®˜æ–¹ç¶²é ï¼Œä½¿ç”¨å’Œè³‡æ–™é›†å°æ‡‰çš„evaluation matrics\\\n",
    "å›å‚³ä¸€å€‹æ¸¬è©¦çµæœçš„dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KZgSNytdJg2P"
   },
   "outputs": [],
   "source": [
    "# å»ºç«‹è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™ (å®šç¾© function)\n",
    "# å°‡ä½œç‚º transformers.Trainer çš„ parameters ä¹‹ä¸€\n",
    "\n",
    "# Scikit-learn çš„ precision_recall_fscore_support å¥—ä»¶å¯ä»¥ä¸€æ¬¡è¨ˆç®— F1 score, precision, å’Œ recall\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics_cola(eval_preds):\n",
    "    # è«‹åƒè€ƒGLUE benchmarkçš„å®˜æ–¹ç¶²é ï¼Œä½¿ç”¨å’Œè³‡æ–™é›†å°æ‡‰çš„evaluation matrics\n",
    "    metric = datasets.load_metric(\"glue\", \"cola\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics_mrpc(eval_preds):\n",
    "    # è«‹åƒè€ƒGLUE benchmarkçš„å®˜æ–¹ç¶²é ï¼Œä½¿ç”¨å’Œè³‡æ–™é›†å°æ‡‰çš„evaluation matrics\n",
    "    metric = datasets.load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: PEFT \n",
    "åœ¨é€™å€‹æ¨¡å¡Šä¸­ï¼Œæˆ‘å€‘è¦æ±‚åŒå­¸å€‘æŠŠæ¨¡å‹æ”¹æˆPEFTçš„å½¢å¼ï¼ˆBitfitã€LoRAæˆ–è€…å…¶ä»–ï¼‰\\\n",
    "åŒå­¸å€‘å¯ä»¥å°‡æ¨¡å‹çš„å¯è¨“ç·´åƒæ•¸é‡printå‡ºä¾†\n",
    "\n",
    "ä»¥ä¸‹æ˜¯å„å€‹è³‡æ–™é›†çš„baseline: \n",
    "\n",
    "|dataset|metrics|baseline|\n",
    "|----|----|----|\n",
    "|CoLA|Matthew's Corr|0.6|\n",
    "|SST2|Accuracy|0.88|\n",
    "|MRPC|Accuracy|0.8|\n",
    "|STSB|Pearson-Spearman Corr|0.8|\n",
    "|QQP|F1 / Accuracy|0.8/0.8|\n",
    "|MNLI_Matched|Accuracy|0.8|\n",
    "|MNLI_Mismatched|Accuracy|0.8|\n",
    "|QNLI|Accuracy|0.85|\n",
    "|RTE|Accuracy|0.7|\n",
    "|WNLI|Accuracy|0.8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "#### LoRA on CoLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 125,829,124 || trainable%: 0.9393326142841144\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 05:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.685670</td>\n",
       "      <td>0.301826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.496303</td>\n",
       "      <td>0.469179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.423300</td>\n",
       "      <td>0.464689</td>\n",
       "      <td>0.480204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.456541</td>\n",
       "      <td>0.565160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.406895</td>\n",
       "      <td>0.585751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.414831</td>\n",
       "      <td>0.588414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.456625</td>\n",
       "      <td>0.583025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.583940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.494482</td>\n",
       "      <td>0.583025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.505460</td>\n",
       "      <td>0.590523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22754/2882079743.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"cola\")\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.35362693001242246, metrics={'train_runtime': 344.7826, 'train_samples_per_second': 248.011, 'train_steps_per_second': 0.986, 'total_flos': 2093805016160880.0, 'train_loss': 0.35362693001242246, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_CoLA\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-3,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=256,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=256,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-4,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "#### LoRA on MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 125,829,124 || trainable%: 0.9393326142841144\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 05:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>0.636725</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.825112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.347224</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.886894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.333836</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.889667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.347127</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.916376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.302864</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.901257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.396562</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.903010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.346742</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.915078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.398173</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.906897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.396105</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.911972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.404473</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2985/2882079743.py:17: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=290, training_loss=0.2997131816272078, metrics={'train_runtime': 342.8572, 'train_samples_per_second': 106.983, 'train_steps_per_second': 0.846, 'total_flos': 1987394748218880.0, 'train_loss': 0.2997131816272078, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_MRPC\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-3,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=128,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=128,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-4,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_mrpc,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_mrpc,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_mrpc,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "#### BitFit on CoLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 102914 || all params: 124647170 || trainable%: 0.08256424915222704\n"
     ]
    }
   ],
   "source": [
    "# BitFit è¨“ç·´è¨­å®š\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if \"bias\" not in name:\n",
    "        param.require_grad = False\n",
    "    else:\n",
    "        param.require_grad = True\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1340' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1340/1340 09:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.539100</td>\n",
       "      <td>0.418387</td>\n",
       "      <td>0.548265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.439593</td>\n",
       "      <td>0.526136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.574695</td>\n",
       "      <td>0.553641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.589701</td>\n",
       "      <td>0.600984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.645020</td>\n",
       "      <td>0.590990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>0.599641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.906343</td>\n",
       "      <td>0.562623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.816503</td>\n",
       "      <td>0.603317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.945155</td>\n",
       "      <td>0.608684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>1.074773</td>\n",
       "      <td>0.595616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1340, training_loss=0.1744336220755506, metrics={'train_runtime': 582.1451, 'train_samples_per_second': 146.888, 'train_steps_per_second': 2.302, 'total_flos': 2064580034754360.0, 'train_loss': 0.1744336220755506, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"BitFit_CoLA\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-4,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=64,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=64,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-5,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "#### BitFit on MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 102914 || all params: 124647170 || trainable%: 0.08256424915222704\n"
     ]
    }
   ],
   "source": [
    "# BitFit è¨“ç·´è¨­å®š\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if \"bias\" not in name:\n",
    "        param.require_grad = False\n",
    "    else:\n",
    "        param.require_grad = True\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='580' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [580/580 08:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.451975</td>\n",
       "      <td>0.776961</td>\n",
       "      <td>0.856693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.356418</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.884477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.866412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>0.870098</td>\n",
       "      <td>0.903108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.359604</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.904594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.907801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.478919</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.913580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.676003</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.684587</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.915194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.715340</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.912343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=580, training_loss=0.1962397330793841, metrics={'train_runtime': 500.8034, 'train_samples_per_second': 73.242, 'train_steps_per_second': 1.158, 'total_flos': 1960341806841600.0, 'train_loss': 0.1962397330793841, 'epoch': 10.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"BitFit_MRPC\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-4,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=64,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=64,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-5,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_mrpc,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_mrpc,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_mrpc,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ¯”è¼ƒä¸åŒhyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éƒ½æ˜¯LoRa on CoLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Batch size = 128\n",
    "åŸæœ¬: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 125,829,124 || trainable%: 0.9393326142841144\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 06:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>0.508521</td>\n",
       "      <td>0.453590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>0.536687</td>\n",
       "      <td>0.452419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.413946</td>\n",
       "      <td>0.575239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.406251</td>\n",
       "      <td>0.596038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.414675</td>\n",
       "      <td>0.603193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.421161</td>\n",
       "      <td>0.604496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.535753</td>\n",
       "      <td>0.585706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.576062</td>\n",
       "      <td>0.593695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.547534</td>\n",
       "      <td>0.603359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.607017</td>\n",
       "      <td>0.590675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.29034543677942076, metrics={'train_runtime': 364.2276, 'train_samples_per_second': 234.771, 'train_steps_per_second': 1.84, 'total_flos': 2093805016160880.0, 'train_loss': 0.29034543677942076, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_CoLA_2\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-3,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=128,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=128,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-4,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Learning Rate = 1e-4\n",
    "åŸæœ¬: 1e-3\n",
    "weight decay è·Ÿè‘—èª¿æ•´, å¾ 1e-4 -> 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 125,829,124 || trainable%: 0.9393326142841144\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 05:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.525661</td>\n",
       "      <td>0.382568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>0.458045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.525648</td>\n",
       "      <td>0.447911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.468637</td>\n",
       "      <td>0.507314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.460892</td>\n",
       "      <td>0.510438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.437740</td>\n",
       "      <td>0.527461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>0.474434</td>\n",
       "      <td>0.520756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>0.462667</td>\n",
       "      <td>0.526840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.469191</td>\n",
       "      <td>0.528618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.452924</td>\n",
       "      <td>0.532110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.4147378388573142, metrics={'train_runtime': 310.0971, 'train_samples_per_second': 275.752, 'train_steps_per_second': 1.096, 'total_flos': 2093805016160880.0, 'train_loss': 0.4147378388573142, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_CoLA_2\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-4,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=256,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=256,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-5,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## èª¿æ•´LoRA rank(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "### r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066137476531099\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 05:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.459850</td>\n",
       "      <td>0.529114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.498871</td>\n",
       "      <td>0.520951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.406159</td>\n",
       "      <td>0.573511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.416802</td>\n",
       "      <td>0.593084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.470340</td>\n",
       "      <td>0.586123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.408497</td>\n",
       "      <td>0.618264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.501184</td>\n",
       "      <td>0.615864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.436887</td>\n",
       "      <td>0.618897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.528743</td>\n",
       "      <td>0.613466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.510608</td>\n",
       "      <td>0.610736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.33557842338786403, metrics={'train_runtime': 307.1694, 'train_samples_per_second': 278.381, 'train_steps_per_second': 1.107, 'total_flos': 2086693561277040.0, 'train_loss': 0.33557842338786403, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_CoLA\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-3,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=256,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=256,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-4,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "wHfKgmDjJg2U",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "8a01ba35-e9ea-4db1-aceb-1e71b633022c"
   },
   "source": [
    "### r = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,771,778 || all params: 126,418,948 || trainable%: 1.4015130073697497\n"
     ]
    }
   ],
   "source": [
    "# LoRA è¨“ç·´è¨­å®š\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    " \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", trust_remote_code=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 05:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.577912</td>\n",
       "      <td>0.390709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.494683</td>\n",
       "      <td>0.526048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.457663</td>\n",
       "      <td>0.545210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.458133</td>\n",
       "      <td>0.580628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.429213</td>\n",
       "      <td>0.608191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.416304</td>\n",
       "      <td>0.591793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.474691</td>\n",
       "      <td>0.593055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.482404</td>\n",
       "      <td>0.593382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.501416</td>\n",
       "      <td>0.613252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.525970</td>\n",
       "      <td>0.605723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/volley114/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.32091039910035973, metrics={'train_runtime': 309.7485, 'train_samples_per_second': 276.063, 'train_steps_per_second': 1.098, 'total_flos': 2108027925928560.0, 'train_loss': 0.32091039910035973, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "\n",
    "# è¨­å®š TrainingArguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"LoRA_CoLA\",        # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
    "    num_train_epochs=10,              # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®\n",
    "    learning_rate=1e-3,              # å­¸ç¿’ç‡\n",
    "    per_device_train_batch_size=256,  # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    per_device_eval_batch_size=256,   # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç©çš„æ­¥æ•¸\n",
    "    warmup_steps=10,                # learning rate scheduler çš„åƒæ•¸\n",
    "    weight_decay=1e-4,               # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡\n",
    "    evaluation_strategy='epoch',     # è¨­å®šé©—è­‰çš„æ™‚æ©Ÿ\n",
    "    save_strategy='epoch',           # è¨­å®šå„²å­˜çš„æ™‚æ©Ÿ\n",
    "    logging_steps=0.1,\n",
    "    seed=100\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # ğŸ¤— çš„æ¨¡å‹\n",
    "    args=training_args,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸\n",
    "    train_dataset=train_dataset_cola,         # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)\n",
    "    eval_dataset=val_dataset_cola,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰\n",
    "    compute_metrics=compute_metrics_cola,     # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´\n",
    "trainer.args._n_gpu=1\n",
    "\n",
    "# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dee68c6ffd43a4b5cf30f09d71bcb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f06d935a23144d0eba94969c6ea91cbb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8e95a576f3d34366beb19b5bde93dc5b",
      "value": "â€‡5000/5000â€‡[00:07&lt;00:00,â€‡657.65â€‡examples/s]"
     }
    },
    "02168fc723b143849b7d8488d4b73997": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08040ed6406841cdb419bc520c27b0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d92e9c53c0cc4b38afefb821b615b01c",
       "IPY_MODEL_08319e9ed1b9480baae1a5d697b8cc10",
       "IPY_MODEL_b86ee9c64f494d7bbefab8b86bc84da3"
      ],
      "layout": "IPY_MODEL_3938d967b7414535ba6b5011dc56470f"
     }
    },
    "08319e9ed1b9480baae1a5d697b8cc10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcdeab999f3b48a9aae8c12420b2bfcb",
      "max": 20000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_daaf705153f7406682246dfe5d7321f7",
      "value": 20000
     }
    },
    "0ea9133bf4664aee8c6dd74eda254cc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dfed01ed6bf4f04994cf26f439380f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ec526cda13940a6b77fdd623b69f1f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fe8a068cd744e8ca846095190e977dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23697ab45e464e31bbf0f7e3883dce11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3299d0d455874ab38b7f3545d9c73e79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_339e040e2baf4dcf900b6d76abeb2c14",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_02168fc723b143849b7d8488d4b73997",
      "value": "Map:â€‡100%"
     }
    },
    "339e040e2baf4dcf900b6d76abeb2c14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3938d967b7414535ba6b5011dc56470f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a5a32b8db02492abf45cc1a5c111d7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_608e362d2f92485a97e3556fd370957b",
       "IPY_MODEL_ec632d4d34544dd989df07c1c26f2467",
       "IPY_MODEL_00dee68c6ffd43a4b5cf30f09d71bcb0"
      ],
      "layout": "IPY_MODEL_0ea9133bf4664aee8c6dd74eda254cc5"
     }
    },
    "409db3f8a55a4cd28696e66074773aab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4938681ec83a4338a20288242b762122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "608e362d2f92485a97e3556fd370957b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fae4e545f58e4ca9aa1f49bad91a36d4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fece13fdff59433ebbb3a35320574e5f",
      "value": "Map:â€‡100%"
     }
    },
    "620d401375d7406e8922d7c51246bd31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6df9c93e3e6e4f94b48b1f2df6b350c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a142a6dd37b46daa64feff35f4168c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fe8a068cd744e8ca846095190e977dc",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6df9c93e3e6e4f94b48b1f2df6b350c3",
      "value": 25000
     }
    },
    "87a9ce7ed8fa43988c59bb3d9ced637b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e95a576f3d34366beb19b5bde93dc5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0bb14037015498794f441bcbac020da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b86ee9c64f494d7bbefab8b86bc84da3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_409db3f8a55a4cd28696e66074773aab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f36b9796a29f45d980986087aeb79d42",
      "value": "â€‡20000/20000â€‡[00:23&lt;00:00,â€‡898.68â€‡examples/s]"
     }
    },
    "b9548eee1f3c45f3b49865e4a625d00f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3299d0d455874ab38b7f3545d9c73e79",
       "IPY_MODEL_7a142a6dd37b46daa64feff35f4168c6",
       "IPY_MODEL_f5de6cf424a54676bb153afa6cfe96a4"
      ],
      "layout": "IPY_MODEL_1ec526cda13940a6b77fdd623b69f1f7"
     }
    },
    "bcdeab999f3b48a9aae8c12420b2bfcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d92e9c53c0cc4b38afefb821b615b01c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dfed01ed6bf4f04994cf26f439380f7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_23697ab45e464e31bbf0f7e3883dce11",
      "value": "Map:â€‡100%"
     }
    },
    "daaf705153f7406682246dfe5d7321f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec632d4d34544dd989df07c1c26f2467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87a9ce7ed8fa43988c59bb3d9ced637b",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4938681ec83a4338a20288242b762122",
      "value": 5000
     }
    },
    "f06d935a23144d0eba94969c6ea91cbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f36b9796a29f45d980986087aeb79d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5de6cf424a54676bb153afa6cfe96a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0bb14037015498794f441bcbac020da",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_620d401375d7406e8922d7c51246bd31",
      "value": "â€‡25000/25000â€‡[00:27&lt;00:00,â€‡846.66â€‡examples/s]"
     }
    },
    "fae4e545f58e4ca9aa1f49bad91a36d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fece13fdff59433ebbb3a35320574e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
